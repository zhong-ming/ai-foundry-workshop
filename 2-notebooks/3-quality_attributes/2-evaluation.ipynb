{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ee0c0ebe",
      "metadata": {},
      "source": [
        "# ðŸ‹ï¸â€â™€ï¸ Health & Fitness Evaluations with Azure AI Foundry ðŸ‹ï¸â€â™‚ï¸\n",
        "\n",
        "This notebook demonstrates how to **evaluate** a Generative AI model (or application) using the **Azure AI Foundry** ecosystem. We'll highlight three key Python SDKs:\n",
        "1. **`azure-ai-projects`** (`AIProjectClient`): manage & orchestrate evaluations in the cloud.\n",
        "2. **`azure-ai-inference`**: perform model inference (optional but helpful if generating data for evaluation).\n",
        "3. **`azure-ai-evaluation`**: run automated metrics for LLM output quality & safety.\n",
        "\n",
        "We'll create or use some synthetic \"health & fitness\" Q&A data, then measure how well your model is answering. We'll do both **local** evaluation and **cloud** evaluation (on an Azure AI Foundry project). \n",
        "\n",
        "> **Disclaimer**: This covers a hypothetical health & fitness scenario. **No real medical advice** is provided. Always consult professionals.\n",
        "\n",
        "## Notebook Contents\n",
        "1. [Setup & Imports](#1-Setup-and-Imports)\n",
        "2. [Mermaid Diagram of the Flow](#2-Mermaid-Diagram)\n",
        "3. [Local Evaluation Examples](#3-Local-Evaluation)\n",
        "4. [Cloud Evaluation with `AIProjectClient`](#4-Cloud-Evaluation)\n",
        "5. [Extra Topics](#5-Extra-Topics)\n",
        "   - [Risk & Safety Evaluators](#5.1-Risk-and-Safety)\n",
        "   - [More Quality Evaluators](#5.2-Quality)\n",
        "   - [Custom Evaluators](#5.3-Custom)\n",
        "   - [Simulators & Adversarial Data](#5.4-Simulators)\n",
        "6. [Conclusion](#6-Conclusion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bfadf84",
      "metadata": {
        "id": "1-Setup-and-Imports"
      },
      "source": [
        "## 1. Setup and Imports\n",
        "We'll install necessary libraries, import them, and define some synthetic data. \n",
        "\n",
        "### Dependencies\n",
        "- `azure-ai-projects` for orchestrating evaluations in your Azure AI Foundry Project.\n",
        "- `azure-ai-evaluation` for built-in or custom metrics (like Relevance, Groundedness, F1Score, etc.).\n",
        "- `azure-ai-inference` (optional) if you'd like to generate completions to produce data to evaluate.\n",
        "- `azure-identity` (for Azure authentication via `DefaultAzureCredential`).\n",
        "\n",
        "### Synthetic Data\n",
        "We'll create a small JSONL with *health & fitness* Q&A pairs, including `query`, `response`, `context`, and `ground_truth`. This simulates a scenario where we have user questions, the model's answers, plus a reference ground truth.\n",
        "\n",
        "You can adapt this approach to any domain: e.g., finance, e-commerce, etc.\n",
        "\n",
        "<img src=\"./seq-diagrams/2-evals.png\" alt=\"Evaluation Flow\" width=\"30%\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8b889daf",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# If you need to install these, uncomment:\n",
        "# !pip install azure-ai-projects azure-ai-evaluation azure-ai-inference azure-identity\n",
        "# !pip install opentelemetry-sdk azure-core-tracing-opentelemetry  # optional for advanced tracing\n",
        "\n",
        "import json\n",
        "import os\n",
        "import uuid\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any\n",
        "\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# We'll create a synthetic dataset in JSON Lines format\n",
        "synthetic_eval_data = [\n",
        "    {\n",
        "        \"query\": \"How can I start a beginner workout routine at home?\",\n",
        "        \"context\": \"Workout routines can include push-ups, bodyweight squats, lunges, and planks.\",\n",
        "        \"response\": \"You can just go for 10 push-ups total.\",\n",
        "        \"ground_truth\": \"At home, you can start with short, low-intensity workouts: push-ups, lunges, planks.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Are diet sodas healthy for daily consumption?\",\n",
        "        \"context\": \"Sugar-free or diet drinks may reduce sugar intake, but they still contain artificial sweeteners.\",\n",
        "        \"response\": \"Yes, diet sodas are 100% healthy.\",\n",
        "        \"ground_truth\": \"Diet sodas have fewer sugars than regular soda, but 'healthy' is not guaranteed due to artificial additives.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What's the capital of France?\",\n",
        "        \"context\": \"France is in Europe. Paris is the capital.\",\n",
        "        \"response\": \"London.\",\n",
        "        \"ground_truth\": \"Paris.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Write them to a local JSONL file\n",
        "eval_data_path = Path(\"./health_fitness_eval_data.jsonl\")\n",
        "with eval_data_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for row in synthetic_eval_data:\n",
        "        f.write(json.dumps(row) + \"\\n\")\n",
        "\n",
        "print(f\"Sample evaluation data written to {eval_data_path.resolve()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da2d5598",
      "metadata": {
        "id": "3-Local-Evaluation"
      },
      "source": [
        "# 3. Local Evaluation Examples\n",
        "\n",
        "We'll show how to run local, code-based evaluation on a JSONL dataset. We'll:\n",
        "1. **Load** the data.\n",
        "2. **Define** one or more evaluators. (e.g. `F1ScoreEvaluator`, `RelevanceEvaluator`, or custom.)\n",
        "3. **Run** `evaluate(...)` to produce a dictionary of metrics.\n",
        "\n",
        "> We can also do multi-turn conversation data or add extra columns like `ground_truth` for advanced metrics.\n",
        "\n",
        "## Example 1: Combining F1Score & Relevance\n",
        "We'll combine:\n",
        "- `F1ScoreEvaluator` (NLP-based, compares `response` to `ground_truth`)\n",
        "- `RelevanceEvaluator` (AI-assisted, uses GPT to judge how well `response` addresses `query`)\n",
        "\n",
        "We'll also show a custom code-based evaluator that logs response length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9f04f13",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from azure.ai.evaluation import (\n",
        "    evaluate,\n",
        "    F1ScoreEvaluator,\n",
        "    RelevanceEvaluator\n",
        ")\n",
        "\n",
        "# Our custom evaluator to measure response length.\n",
        "def response_length_eval(response, **kwargs):\n",
        "    return {\"resp_length\": len(response)}\n",
        "\n",
        "# We'll define an example GPT-based config (if we want Relevance to run). \n",
        "# This is needed for AI-assisted evaluators. Fill with your Azure OpenAI config.\n",
        "# If you skip Relevance, you can omit.\n",
        "model_config = {\n",
        "    \"azure_endpoint\": os.environ.get(\"AOAI_ENDPOINT\", \"https://dummy-endpoint.azure.com\"),\n",
        "    \"api_key\": os.environ.get(\"AOAI_API_KEY\", \"fake-key\"),\n",
        "    \"azure_deployment\": os.environ.get(\"AOAI_DEPLOYMENT\", \"gpt-4\"),\n",
        "    \"api_version\": os.environ.get(\"AOAI_API_VERSION\", \"2023-07-01-preview\"),\n",
        "}\n",
        "\n",
        "f1_eval = F1ScoreEvaluator()\n",
        "rel_eval = RelevanceEvaluator(model_config=model_config)\n",
        "\n",
        "# We'll run evaluate(...) with these evaluators.\n",
        "results = evaluate(\n",
        "    data=str(eval_data_path),\n",
        "    evaluators={\n",
        "        \"f1_score\": f1_eval,\n",
        "        \"relevance\": rel_eval,\n",
        "        \"resp_len\": response_length_eval\n",
        "    },\n",
        "    evaluator_config={\n",
        "        \"f1_score\": {\n",
        "            \"column_mapping\": {\n",
        "                \"response\": \"${data.response}\",\n",
        "                \"ground_truth\": \"${data.ground_truth}\"\n",
        "            }\n",
        "        },\n",
        "        \"relevance\": {\n",
        "            \"column_mapping\": {\n",
        "                \"query\": \"${data.query}\",\n",
        "                \"response\": \"${data.response}\"\n",
        "            }\n",
        "        },\n",
        "        \"resp_len\": {\n",
        "            \"column_mapping\": {\n",
        "                \"response\": \"${data.response}\"\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    # (Optional) Provide azure_ai_project or output_path.\n",
        ")\n",
        "\n",
        "print(\"Local evaluation result =>\")\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d525400",
      "metadata": {},
      "source": [
        "**Inspecting Local Results**\n",
        "\n",
        "The `evaluate(...)` call returns a dictionary with:\n",
        "- **`metrics`**: aggregated metrics across rows (like average F1 or Relevance)\n",
        "- **`rows`**: row-by-row results with inputs and evaluator outputs\n",
        "- **`traces`**: debugging info (if any)\n",
        "\n",
        "You can do further analysis, store results in a database, or use them as part of your CI/CD pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7b903ea",
      "metadata": {
        "id": "4-Cloud-Evaluation"
      },
      "source": [
        "# 4. Cloud Evaluation with `AIProjectClient`\n",
        "\n",
        "Sometimes, we want to:\n",
        "- Evaluate large or sensitive datasets in the cloud (scalability, governed access).\n",
        "- Keep track of evaluation results in an Azure AI Foundry project.\n",
        "- Optionally schedule recurring evaluations.\n",
        "\n",
        "We'll do that by:\n",
        "1. **Upload** the local JSONL to your Azure AI Foundry project.\n",
        "2. **Create** an `Evaluation` referencing built-in or custom evaluator definitions.\n",
        "3. **Poll** until the job is done.\n",
        "4. **Review** the results in the portal or via `project_client.evaluations.get(...)`.\n",
        "\n",
        "### Prerequisites\n",
        "- Azure AI Foundry project with a valid **Connection String**. (See your project's Overview page.)\n",
        "- A GPT-based Azure OpenAI deployment if you want AI-assisted metrics like Relevance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a99ccc1",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install azure-ai-ml --quiet --no-cache-dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68d936ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from azure.ai.projects import AIProjectClient\n",
        "from azure.ai.projects.models import (\n",
        "    Evaluation, Dataset, EvaluatorConfiguration, ConnectionType\n",
        ")\n",
        "from azure.ai.evaluation import F1ScoreEvaluator, RelevanceEvaluator, ViolenceEvaluator\n",
        "\n",
        "# 1) Connect to Azure AI Foundry project\n",
        "project_conn_str = os.environ.get(\"PROJECT_CONNECTION_STRING\")\n",
        "credential = DefaultAzureCredential()\n",
        "\n",
        "project_client = AIProjectClient.from_connection_string(\n",
        "    credential=credential,\n",
        "    conn_str=project_conn_str\n",
        ")\n",
        "print(\"âœ… Created AIProjectClient.\")\n",
        "\n",
        "# 2) Upload data for evaluation\n",
        "uploaded_data_id, _ = project_client.upload_file(str(eval_data_path))\n",
        "print(\"âœ… Uploaded JSONL to project. Data asset ID:\", uploaded_data_id)\n",
        "\n",
        "# 3) Prepare an Azure OpenAI connection for AI-assisted evaluators\n",
        "default_conn = project_client.connections.get_default(ConnectionType.AZURE_OPEN_AI)\n",
        "\n",
        "deployment_name = os.environ.get(\"AOAI_DEPLOYMENT\", \"gpt-4\")\n",
        "api_version = os.environ.get(\"AOAI_API_VERSION\", \"2023-07-01-preview\")\n",
        "\n",
        "# 4) Construct the evaluation object\n",
        "model_config = default_conn.to_evaluator_model_config(\n",
        "    deployment_name=deployment_name,\n",
        "    api_version=api_version\n",
        ")\n",
        "\n",
        "evaluation = Evaluation(\n",
        "    display_name=\"Health Fitness Remote Evaluation\",\n",
        "    description=\"Evaluating dataset for correctness.\",\n",
        "    data=Dataset(id=uploaded_data_id),\n",
        "    evaluators={\n",
        "        # We'll do F1Score (NLP-based) and Relevance (AI-assisted), plus a Violence check for safety.\n",
        "        \"f1_score\": EvaluatorConfiguration(id=F1ScoreEvaluator.id),\n",
        "        \"relevance\": EvaluatorConfiguration(\n",
        "            id=RelevanceEvaluator.id,\n",
        "            init_params={\"model_config\": model_config}\n",
        "        ),\n",
        "        \"violence\": EvaluatorConfiguration(\n",
        "            id=ViolenceEvaluator.id,\n",
        "            init_params={\"azure_ai_project\": project_client.scope}\n",
        "        ),\n",
        "    },\n",
        ")\n",
        "\n",
        "# 5) Create & track the evaluation\n",
        "cloud_eval = project_client.evaluations.create(\n",
        "    evaluation=evaluation,\n",
        ")\n",
        "print(\"âœ… Created evaluation job. ID:\", cloud_eval.id)\n",
        "\n",
        "# 6) Poll or fetch final status\n",
        "fetched_eval = project_client.evaluations.get(cloud_eval.id)\n",
        "print(\"Current status:\", fetched_eval.status)\n",
        "if hasattr(fetched_eval, 'properties'):\n",
        "    link = fetched_eval.properties.get(\"AiStudioEvaluationUri\", \"\")\n",
        "    if link:\n",
        "        print(\"View details in Foundry:\", link)\n",
        "else:\n",
        "    print(\"No link found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "091290cd",
      "metadata": {},
      "source": [
        "### Viewing Cloud Evaluation Results\n",
        "- You can navigate to the **Evaluations** tab in your AI Foundry project to see your new evaluation.\n",
        "- Filter or open it to see aggregated metrics & row-level details.\n",
        "- If you use risk & safety evaluators (like `ViolenceEvaluator`, `SexualEvaluator`, `HateUnfairnessEvaluator`), you'll see how many responses had severe content.\n",
        "- For AI-assisted quality evaluators (like `relevance`, `groundedness`, `coherence`), you'll see average scores plus per-row breakdown.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55e3a2c4",
      "metadata": {},
      "source": [
        "# 5. Extra Topics\n",
        "We'll do a quick overview of some advanced features:\n",
        "1. [Risk & Safety Evaluators](#5.1-Risk-and-Safety)\n",
        "2. [Additional Quality Evaluators](#5.2-Quality)\n",
        "3. [Custom Evaluators](#5.3-Custom)\n",
        "4. [Simulators & Adversarial Data](#5.4-Simulators)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7490e0eb",
      "metadata": {
        "id": "5.1-Risk-and-Safety"
      },
      "source": [
        "## 5.1 Risk & Safety Evaluators\n",
        "\n",
        "Azure AI Foundry includes built-in evaluators that use a specialized safety service to detect content risks. Examples:\n",
        "- **ViolenceEvaluator**: detects violent or harmful content.\n",
        "- **SexualEvaluator**: checks if text contains sexual or explicit references.\n",
        "- **HateUnfairnessEvaluator**: checks for hateful content.\n",
        "- **SelfHarmEvaluator**: detects instructions or content about self-harm.\n",
        "- **ProtectedMaterialEvaluator**: detects copyright or protected text in the response.\n",
        "\n",
        "These typically accept a `query` and `response` string, and produce a severity label (like \"Very low\", \"Low\", \"Medium\", \"High\"), plus a reason. They also produce a numeric severity score in `violence_score`, `sexual_score`, etc.\n",
        "\n",
        "### Region Availability\n",
        "Currently, these risk evaluators are primarily available in **East US 2**, **France Central**, **Sweden Central**, **Switzerland West** (some region exceptions for protected material). Make sure your project is in a supported region.\n",
        "\n",
        "### Usage\n",
        "```python\n",
        "from azure.ai.evaluation import ViolenceEvaluator\n",
        "\n",
        "violence_eval = ViolenceEvaluator(\n",
        "    credential=DefaultAzureCredential(),\n",
        "    azure_ai_project={\n",
        "        \"subscription_id\": \"...\",\n",
        "        \"resource_group_name\": \"...\",\n",
        "        \"project_name\": \"...\"\n",
        "    }\n",
        ")\n",
        "\n",
        "row_result = violence_eval(\n",
        "    query=\"What is the capital of France?\",\n",
        "    answer=\"Paris.\"\n",
        ")\n",
        "print(row_result)\n",
        "# => {'violence': 'Very low', 'violence_score': 0, 'violence_reason': 'No violent content found.'}\n",
        "```\n",
        "\n",
        "You can combine these with `evaluate(...)` locally or in the cloud. For example:\n",
        "```python\n",
        "result = evaluate(\n",
        "    data=\"./mydata.jsonl\",\n",
        "    evaluators={\n",
        "        \"violence\": violence_eval\n",
        "    },\n",
        "    evaluator_config={\n",
        "        \"violence\": {\n",
        "            \"column_mapping\": {\n",
        "                \"query\": \"${data.query}\",\n",
        "                \"response\": \"${data.response}\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        ")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92a94f46",
      "metadata": {
        "id": "5.2-Quality"
      },
      "source": [
        "## 5.2 Additional Quality Evaluators\n",
        "Beyond `F1Score` and `Relevance`, there are many built-ins:\n",
        "- **GroundednessEvaluator** (AI-assisted) checks if `response` is grounded in `context`.\n",
        "- **CoherenceEvaluator** measures how logically the response is written.\n",
        "- **FluencyEvaluator** measures grammatical correctness.\n",
        "- **SimilarityEvaluator**, **RougeScoreEvaluator**, **BleuScoreEvaluator**, etc. for comparing to ground truths.\n",
        "\n",
        "**AI-Assisted** metrics (like `GroundednessEvaluator`) require a GPT model config or your `azure_ai_project` if using GroundednessPro or risk-based metrics.\n",
        "\n",
        "```python\n",
        "from azure.ai.evaluation import GroundednessEvaluator\n",
        "\n",
        "g_eval = GroundednessEvaluator(model_config)\n",
        "res = g_eval(\n",
        "    query=\"Are diet sodas healthy?\",\n",
        "    context=\"Diet sodas have fewer sugars, but can contain artificial sweeteners.\",\n",
        "    response=\"Yes, they are extremely healthy with no caveats.\"\n",
        ")\n",
        "print(res)\n",
        "# => {'groundedness': 2.0, 'groundedness_reason': \"The response is partially related...\"}\n",
        "```\n",
        "When used in a dataset, these produce an average groundedness score.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd6f718e",
      "metadata": {
        "id": "5.3-Custom"
      },
      "source": [
        "## 5.3 Custom Evaluators\n",
        "You can build your own code-based or prompt-based evaluators. For example, a code-based function that simply checks if the response includes certain keywords, or a large language model-based approach with your own prompt template. Then you can either:\n",
        "1. Use them locally with `evaluate(...)`.\n",
        "2. **Register** them to your Azure AI Foundry project if you want to use them in the cloud.\n",
        "\n",
        "### Example: Code-based\n",
        "```python\n",
        "class AnswerLengthEvaluator:\n",
        "    def __call__(self, response: str, **kwargs):\n",
        "        return {\"answer_length\": len(response)}\n",
        "```\n",
        "Then pass `AnswerLengthEvaluator()` in the `evaluators` dict. \n",
        "\n",
        "### Example: Prompt-based\n",
        "You can create a `.prompty` file describing how to judge the response for \"friendliness\" or other custom metrics. Load it via `promptflow.client.load_flow(...)`, then call it in a custom class.\n",
        "\n",
        "```python\n",
        "# friend_eval.py\n",
        "from promptflow.client import load_flow\n",
        "\n",
        "class FriendlinessEvaluator:\n",
        "    def __init__(self, model_config):\n",
        "        self._flow = load_flow(source=\"friendliness.prompty\", model={\"configuration\": model_config})\n",
        "\n",
        "    def __call__(self, response: str, **kwargs):\n",
        "        return self._flow(response=response)\n",
        "```\n",
        "This can be integrated into `evaluate(...)` or packaged and registered to your AI Foundry environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bde67f1f",
      "metadata": {
        "id": "5.4-Simulators"
      },
      "source": [
        "## 5.4 Simulators & Adversarial Data\n",
        "**No real test data?** You can generate your own using the `azure-ai-evaluation` **Simulator**. This simulates user queries (non-adversarial or adversarial) to your AI endpoint, producing data you can then evaluate.\n",
        "\n",
        "### Non-adversarial Simulation\n",
        "- `Simulator` can generate typical queries for your domain (like from a Wikipedia article) and capture your model responses.\n",
        "\n",
        "### Adversarial Simulation\n",
        "- `AdversarialSimulator` tries to produce hateful, sexual, or malicious queries, revealing whether your model outputs unsafe content.\n",
        "- `DirectAttackSimulator` and `IndirectAttackSimulator` help test **jailbreak** scenarios.\n",
        "\n",
        "**Usage**:\n",
        "```python\n",
        "from azure.ai.evaluation.simulator import Simulator, AdversarialSimulator, AdversarialScenario\n",
        "\n",
        "async def my_callback(messages, stream=False, session_state=None, context=None):\n",
        "    # messages is a list in OpenAI chat format\n",
        "    user_msg = messages[\"messages\"][-1][\"content\"]\n",
        "    # call your LLM endpoint, produce a response\n",
        "    # return it in the same chat structure.\n",
        "    return {\"messages\": [...], \"stream\": stream, \"session_state\": session_state}\n",
        "\n",
        "sim = AdversarialSimulator(azure_ai_project={...}, credential=DefaultAzureCredential())\n",
        "outputs = await sim(\n",
        "    scenario=AdversarialScenario.ADVERSARIAL_QA,\n",
        "    target=my_callback,\n",
        "    max_simulation_results=5,\n",
        ")\n",
        "jsonl_data = outputs.to_eval_qa_json_lines()\n",
        "# Then evaluate that with risk & safety evaluators.\n",
        "```\n",
        "\n",
        "This helps you do systematic red-teaming or reliability checks before production!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f63eee3",
      "metadata": {
        "id": "6-Conclusion"
      },
      "source": [
        "# 6. Conclusion ðŸ\n",
        "\n",
        "We covered:\n",
        "1. **Local** evaluations with `evaluate(...)` on JSONL data.\n",
        "2. **Cloud** evaluations with `AIProjectClient`, storing results in Azure AI Foundry.\n",
        "3. Built-in **risk & safety** and **quality** evaluators.\n",
        "4. **Custom** evaluators for advanced scenarios.\n",
        "5. **Simulators** for generating test data or adversarial challenges.\n",
        "\n",
        "**Next Steps**:\n",
        "- Adjust your prompts, model, or application to address issues found in the evaluation.\n",
        "- Combine with **Observability** (tracing) for deeper debugging.\n",
        "- Integrate these evaluations in your **CI/CD** pipelines.\n",
        "- If your domain is specialized, build a **custom evaluator** or tweak built-in prompts.\n",
        "\n",
        "> **Best of luck** building robust AI solutions with Azure AI Foundry's Evaluation capabilities!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
