{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "29901878",
      "metadata": {},
      "source": [
        "# üèãÔ∏è‚Äç‚ôÄÔ∏è Health & Fitness Evaluations with Azure AI Foundry üèãÔ∏è‚Äç‚ôÇÔ∏è\n",
        "\n",
        "This notebook demonstrates how to **evaluate** a Generative AI model using the **Azure AI Foundry** ecosystem. We'll highlight the interplay of three key SDKs:\n",
        "\n",
        "1. **`azure-ai-projects`** (`AIProjectClient`): to manage & orchestrate evaluations from the cloud.\n",
        "2. **`azure-ai-inference`**: to perform model inference (optional, but relevant if you want to generate responses for evaluation).\n",
        "3. **`azure-ai-evaluation`**: to run automated metrics for LLM output quality & safety.\n",
        "\n",
        "We'll create or use some synthetic *health & fitness* Q&A data, then measure how well your model is answering. We'll do a **local** evaluation and a **cloud** evaluation on an **Azure AI Project**. üöÄ\n",
        "\n",
        "## üçâ Notebook Contents\n",
        "1. [Setup & Imports](#1-Setup-and-Imports)\n",
        "2. [Mermaid Diagram of the Flow](#2-Mermaid-Diagram)\n",
        "3. [Local Evaluation Example](#3-Local-Evaluation)\n",
        "4. [Cloud Evaluation with `AIProjectClient`](#4-Cloud-Evaluation)\n",
        "5. [Conclusion](#5-Conclusion)\n",
        "\n",
        "## ‚ö†Ô∏èDisclaimer\n",
        "> This notebook deals with a hypothetical **health & fitness** scenario. **No real medical advice** is provided. Always seek professional guidance when needed!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf2060ce",
      "metadata": {
        "id": "1-Setup-and-Imports"
      },
      "source": [
        "## 1. Setup and Imports\n",
        "We'll install necessary libraries, import them, and define some synthetic data. \n",
        "\n",
        "### Dependencies\n",
        "- `azure-ai-projects` (manages project-based evaluations in the cloud)\n",
        "- `azure-ai-evaluation` (provides built-in metrics like `F1ScoreEvaluator`, `RelevanceEvaluator`, etc.)\n",
        "- `azure-ai-inference` (optionally used if you want to generate completions/chats to produce data to evaluate)\n",
        "- `azure-identity` (for Azure authentication)\n",
        "- `opentelemetry-sdk` and `azure-core-tracing-opentelemetry` if you want to enable advanced tracing (optional).\n",
        "\n",
        "### Synthetic Data\n",
        "We'll create a small JSONL with *health & fitness* Q&A pairs for demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b21da07c",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# If you need to install these, uncomment:\n",
        "# !pip install azure-ai-projects azure-ai-evaluation azure-ai-inference azure-identity\n",
        "\n",
        "import json\n",
        "import os\n",
        "import uuid\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any\n",
        "\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# We'll create a synthetic dataset in JSON Lines format\n",
        "synthetic_eval_data = [\n",
        "    {\n",
        "        \"query\": \"How can I start a beginner workout routine at home?\",\n",
        "        \"context\": \"Workout routines can include push-ups, bodyweight squats, lunges, and planks.\",\n",
        "        \"response\": \"You can just go for 10 push-ups total.\",\n",
        "        \"ground_truth\": \"At home, you can start with short, low-intensity workouts. Examples: push-ups, lunges, and planks in short sets.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Are diet sodas healthy for daily consumption?\",\n",
        "        \"context\": \"Sugar-free or diet drinks may reduce sugar intake, but they still contain artificial sweeteners.\",\n",
        "        \"response\": \"Yes, diet sodas are 100% healthy.\",\n",
        "        \"ground_truth\": \"Diet sodas are lower in sugar than regular soda, but they're not necessarily 'healthy' for daily consumption due to artificial additives.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What's the capital of France?\",\n",
        "        \"context\": \"France is a country in Europe. Paris is the capital.\",\n",
        "        \"response\": \"London.\",\n",
        "        \"ground_truth\": \"Paris.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Write them to a local JSONL file\n",
        "eval_data_path = Path(\"./health_fitness_eval_data.jsonl\")\n",
        "with eval_data_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for row in synthetic_eval_data:\n",
        "        f.write(json.dumps(row) + \"\\n\")\n",
        "\n",
        "print(f\"Sample evaluation data written to {eval_data_path.resolve()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01cd6ec0",
      "metadata": {
        "id": "2-Mermaid-Diagram"
      },
      "source": [
        "<img src=\"./seq-diagrams/2-evals.png\" width=\"50%\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c00480fe",
      "metadata": {
        "id": "3-Local-Evaluation"
      },
      "source": [
        "# 3. Local Evaluation\n",
        "\n",
        "We'll show how to run local, code-based evaluation. Specifically, we'll combine a couple of built-in evaluators:\n",
        "- [**F1ScoreEvaluator**](https://aka.ms/azureaieval-python-ref/f1score)\n",
        "- [**RelevanceEvaluator**](https://aka.ms/azureaieval-python-ref/relevance)\n",
        "\n",
        "Then we'll see how they do on each row in our synthetic data.\n",
        "\n",
        "## Steps\n",
        "1. Import the evaluators\n",
        "2. Construct a local `evaluate(...)` run specifying each evaluator.\n",
        "3. Inspect results.\n",
        "\n",
        "### Note\n",
        "For RelevanceEvaluator (and other AI-assisted evaluators like Groundedness or Coherence), we need a GPT model config. We'll skip real model endpoints here, but the code is shown.\n",
        "\n",
        "We'll also show how we can do code-based custom evaluators (like a simple function that checks length!)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac5e5d66",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from azure.ai.evaluation import (\n",
        "    evaluate,\n",
        "    F1ScoreEvaluator,\n",
        "    RelevanceEvaluator\n",
        ")\n",
        "\n",
        "# We'll define a dummy custom evaluator that just logs the length of the response.\n",
        "def response_length_eval(response, **kwargs):\n",
        "    return {\"resp_length\": len(response)}\n",
        "\n",
        "# If you have a real Azure OpenAI model, fill in:\n",
        "my_model_config = {\n",
        "    \"azure_endpoint\": os.environ.get(\"MY_AOAI_ENDPOINT\", \"https://dummy-endpoint.azure.com/\"),\n",
        "    \"api_key\": os.environ.get(\"MY_AOAI_KEY\", \"fake-key\"),\n",
        "    \"azure_deployment\": os.environ.get(\"MY_AOAI_DEPLOYMENT_NAME\", \"gpt-4\"),\n",
        "    \"api_version\": os.environ.get(\"MY_AOAI_API_VERSION\", \"2023-03-15-preview\"),\n",
        "}\n",
        "\n",
        "# Let's instantiate them\n",
        "f1_eval = F1ScoreEvaluator()  # no GPT needed\n",
        "relevance_eval = RelevanceEvaluator(model_config=my_model_config)  # GPT-based\n",
        "\n",
        "result = evaluate(\n",
        "    data=str(eval_data_path),\n",
        "    evaluators={\n",
        "        \"f1_score\": f1_eval,\n",
        "        \"relevance\": relevance_eval,\n",
        "        \"resp_len\": response_length_eval\n",
        "    },\n",
        "    # We can specify how to map the columns from our data to the arguments of each evaluator.\n",
        "    evaluator_config={\n",
        "        \"f1_score\": {\n",
        "            \"column_mapping\": {\n",
        "                \"response\": \"${data.response}\",\n",
        "                \"ground_truth\": \"${data.ground_truth}\"\n",
        "            }\n",
        "        },\n",
        "        \"relevance\": {\n",
        "            \"column_mapping\": {\n",
        "                \"query\": \"${data.query}\",\n",
        "                \"response\": \"${data.response}\"\n",
        "            }\n",
        "        },\n",
        "        \"resp_len\": {\n",
        "            \"column_mapping\": {\n",
        "                \"response\": \"${data.response}\"\n",
        "            }\n",
        "        },\n",
        "    },\n",
        "    # We won't specify azure_ai_project or output_path to keep it local.\n",
        ")\n",
        "\n",
        "print(\"Local evaluation result =>\\n\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "935ff841",
      "metadata": {},
      "source": [
        "### Inspecting Local Results\n",
        "The `evaluate(...)` function returns a dictionary with:\n",
        "- **`metrics`**: aggregated metrics across rows (like average `f1_score` or average `relevance`)\n",
        "- **`rows`**: row-by-row results with inputs and the computed evaluator outputs\n",
        "- **`traces`**: if you had debugging or additional info\n",
        "\n",
        "Example:\n",
        "```python\n",
        "{\n",
        "  'metrics': { ... },\n",
        "  'rows': [\n",
        "     {\n",
        "       'inputs.response': 'Yes, diet sodas are 100% healthy.',\n",
        "       'outputs.f1_score.f1_score': 0.0,\n",
        "       'outputs.relevance.relevance': 3.0,\n",
        "       'outputs.resp_len.resp_length': 31,\n",
        "       ...\n",
        "     },\n",
        "     ...\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "Our `result` object can be used for analysis or exported to another location."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97f624ac",
      "metadata": {
        "id": "4-Cloud-Evaluation"
      },
      "source": [
        "# 4. Azure Evaluation with `AIProjectClient`\n",
        "\n",
        "Sometimes, we want to scale our evaluation to the cloud, track the results in an **Azure AI Project**, or schedule recurring evaluations. We'll do that by using:\n",
        "- `AIProjectClient` from `azure-ai-projects`\n",
        "- `Evaluation` from `azure.ai.projects.models`.\n",
        "\n",
        "We'll show how you might:\n",
        "1. **Upload** the local JSONL to your Azure AI Project\n",
        "2. **Create** an `Evaluation` referencing a built-in evaluator\n",
        "3. **Submit** the evaluation & poll for results\n",
        "4. **Fetch** & check the final status and (optionally) get a link to AI Studio.\n",
        "\n",
        "## Prerequisites\n",
        "- Azure AI Foundry (AI Hub / Project) with a project-level connection string.\n",
        "- A GPT-based Azure OpenAI deployment if you want to do GPT-based evaluators like Relevance.\n",
        "  \n",
        "### Let's demonstrate now. üéâ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1e62435",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required package\n",
        "!pip install azure-ai-ml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25095dbb",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:opentelemetry.exporter.otlp.proto.http.trace_exporter:Transient error INTERNAL SERVER ERROR encountered while exporting span batch, retrying in 32s.\n",
            "WARNING:opentelemetry.exporter.otlp.proto.http.trace_exporter:Transient error INTERNAL SERVER ERROR encountered while exporting span batch, retrying in 32s.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from azure.ai.projects import AIProjectClient\n",
        "from azure.ai.projects.models import (\n",
        "    Evaluation, Dataset, EvaluatorConfiguration, ConnectionType\n",
        ")\n",
        "from azure.ai.evaluation import F1ScoreEvaluator, RelevanceEvaluator, ViolenceEvaluator\n",
        "\n",
        "# We'll show a minimal example, referencing code from sample.\n",
        "\n",
        "# 1. Connect to Azure AI Project\n",
        "project_connection_string = os.environ.get(\"PROJECT_CONNECTION_STRING\", \"<YOUR_CONNECTION_STRING>\")\n",
        "credential = DefaultAzureCredential()\n",
        "\n",
        "project_client = AIProjectClient.from_connection_string(\n",
        "    credential=credential,\n",
        "    conn_str=project_connection_string,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Created AIProjectClient.\")\n",
        "\n",
        "# 2. Upload data for evaluation\n",
        "data_id, _ = project_client.upload_file(str(eval_data_path))\n",
        "print(\"‚úÖ Uploaded local JSONL. Data asset ID:\", data_id)\n",
        "\n",
        "# 3. Let's define a connection for GPT-based evaluator (Relevance). We'll assume you have a default AOAI conn.\n",
        "default_connection = project_client.connections.get_default(connection_type=ConnectionType.AZURE_OPEN_AI)\n",
        "deployment_name = os.environ.get(\"DEPLOYMENT_NAME\", \"gpt-4\")\n",
        "api_version = os.environ.get(\"AOAI_API_VERSION\", \"2023-06-01-preview\")\n",
        "\n",
        "# 4. Construct the evaluation object\n",
        "evaluation = Evaluation(\n",
        "    display_name=\"Health Fitness Remote Evaluation\",\n",
        "    description=\"Evaluating dataset for correctness and violence.\",\n",
        "    data=Dataset(id=data_id),\n",
        "    evaluators={\n",
        "        # NLP-based\n",
        "        \"f1_score\": EvaluatorConfiguration(\n",
        "            id=F1ScoreEvaluator.id,\n",
        "        ),\n",
        "        # GPT-based\n",
        "        \"relevance\": EvaluatorConfiguration(\n",
        "            id=RelevanceEvaluator.id,\n",
        "            init_params={\n",
        "                \"model_config\": default_connection.to_evaluator_model_config(\n",
        "                    deployment_name=deployment_name, api_version=api_version\n",
        "                )\n",
        "            },\n",
        "        ),\n",
        "        # Safety-based (violence)\n",
        "        \"violence\": EvaluatorConfiguration(\n",
        "            id=ViolenceEvaluator.id,\n",
        "            init_params={\"azure_ai_project\": project_client.scope},\n",
        "        ),\n",
        "    },\n",
        ")\n",
        "\n",
        "evaluation_response = project_client.evaluations.create(\n",
        "    evaluation=evaluation,\n",
        ")\n",
        "print(\"‚úÖ Created evaluation job. ID:\", evaluation_response.id)\n",
        "\n",
        "# Optionally, we can fetch the status.\n",
        "get_evaluation_response = project_client.evaluations.get(evaluation_response.id)\n",
        "print(\"Current status:\", get_evaluation_response.status)\n",
        "\n",
        "# The evaluation may still be in progress. We can poll or just wait.\n",
        "print(\"You can check the Azure AI Project UI to see the final results!\")\n",
        "if isinstance(get_evaluation_response.properties, dict):\n",
        "    print(\"AI Studio link:\", get_evaluation_response.properties.get(\"AiStudioEvaluationUri\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b09de21e",
      "metadata": {},
      "source": [
        "### Observing AI Foundry Evaluation Results\n",
        "In the output logs, you'll see an `AiStudioEvaluationUri` that links to your Azure AI Project in the Azure portal. There, you can:\n",
        "- View aggregated metrics (like average F1 Score or average Relevance Score)\n",
        "- Inspect row-level details to see which queries had the highest or lowest performance.\n",
        "\n",
        "Once the job completes, the final status is `Succeeded`, `Failed`, or `Cancelled`. You can store these metrics for auditing or continuous improvement.\n",
        "\n",
        "## Scheduling Evaluations\n",
        "Using `AIProjectClient`, you can also schedule recurring evaluations (e.g., daily) on new or streaming data. Check out `EvaluationSchedule` in the docs for more advanced usage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78d6e583",
      "metadata": {
        "id": "5-Conclusion"
      },
      "source": [
        "# 5. Conclusion üèÅ\n",
        "\n",
        "In this notebook, we:\n",
        "1. Created *synthetic* health & fitness Q&A data.\n",
        "2. Ran a **local evaluation** with the `evaluate()` function from `azure-ai-evaluation`.\n",
        "3. Demonstrated a **cloud evaluation** using the `AIProjectClient` to create an `Evaluation`.\n",
        "\n",
        "## Next Steps & Tips\n",
        "- Add **OpenTelemetry** to trace your calls for advanced debugging.\n",
        "- Combine more metrics: `GroundednessEvaluator`, `SelfHarmEvaluator`, etc. for a thorough analysis.\n",
        "- Create your own **custom** code-based or prompt-based evaluators to handle domain-specific success metrics.\n",
        "- Explore the **Adversarial** or **Simulator** features in `azure-ai-evaluation` to generate test data.\n",
        "\n",
        "## Resources\n",
        "- [azure-ai-evaluation Documentation](https://aka.ms/azureaieval-python-ref)\n",
        "- [azure-ai-projects Documentation](https://aka.ms/azure-aiprojects)\n",
        "- [azure-ai-inference Documentation](https://aka.ms/azure-ai-inference)\n",
        "\n",
        "Thanks for following along ‚Äì keep building healthy, high-quality AI apps! üçè‚ú®"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
